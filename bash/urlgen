#!/bin/bash
#script to generate URLs to all the subdirectories given url to parent directory
#assumes that a username and password is required

folders_names=$1 #.txt file containing names of subfolders in parent directory
files_names=$2 #.txt file containing names of files in parent directory
weburl=$3 #webDAV url
username=$4
pass=$5
download_dest=$6 #specify where to save files, indicate relative to the root directory (ex. /data/NDAR/donders/MOUS)
outputfile=$7 #name of the output .txt file which will contain all the wget commands

prefix="-P "

if [ ! -z $6 ]
then
	dl_path=$prefix$download_dest
else
	dl_path=
fi

#build wget commands for folders (will recursively fetch all files in the folders) in dataset
while read p; do
	urlbuilt=$weburl/$p/
	echo "wget -r -R \"index.html*\" --no-parent \"$urlbuilt\" --user=$username --password=$pass $dl_path" >> $outputfile
done < $folders_names


#build wget commands for standalone files located in parent directory of dataset
while read p; do
	urlbuilt=$weburl/$p
	echo "wget -r -R \"index.html*\" --no-parent \"$urlbuilt\" --user=$username --password=$pass $dl_path" >> $outputfile
done < $files_names